# 强化学习纪要

- *γ*是折扣因子，用于确定延迟回报与立即回报的相对比例， 越大表明延迟回报的重要程度越高。

 其中α是学习率，一般取（0，1）之间的数，学习率越大，收敛速度越快，但可能会导致过拟合问题。



Q-learning		framework：

生成空矩阵（行：所有可能的状态；列：所有可能动作）

根据当前状态选择可执行的动作

根据选择的动作得到与环境交互得到奖励与到达的下一个状态

根据得到的3种结果计算并更新Q表（选择的动作得到的奖励、Q表中当前状态对应动作的值、下一个状态中可能的最大值）

将下一个状态标记为当前状态

循环知道到达目标地点

###### 各种学习之间的关系与区别

强化学习：没有数据集，放在一个陌生的环境通过不断的试错得到较好的结果。

深度学习：有数据集，有什么学什么。

监督学习：训练时给予带有人工标签的模型，比如说告诉你这个东西是好的，你去学吧。

无监督学习：不知道模型的好坏。



##### 马尔可夫决策过程 (Markov Decision Process，MDP)

![image-20211126170802586](C:\Users\622\AppData\Roaming\Typora\typora-user-images\image-20211126170802586.png)

马尔可夫决策过程定义为：（S, A, P, R, γ）

S：有限状态集

A：有限动作集

P：状态转移概率

R：回报函数

γ：折扣因子，计算累计回报
